* Minimizing the Kullback-Leibler (KL) divergence between a data distribution and an embedded distribution has been used for data visualization and dimensionality reduction. T-SNE, for instance, is a non-parametric algorithm in this school and a parametric variant of t-SNE uses deep neural network to parametrize the embedding. The complexity of t-SNE is O(n 2 ), where n is the number of data points, but it can be approximated in O(n log n).
* The proposed algorithm (DEC) clusters data by simultaneously learning a set of k cluster centers {µj ∈ Z} k j=1 in the feature space Z and the parameters θ of the DNN that maps data points into Z. DEC has two phases: (1) parameter initialization with a deep autoencoder (Vincent et al., 2010) and (2) parameter optimization (i.e., clustering), where we iterate between computing an auxiliary target distribution  and minimizing the Kullback–Leibler (KL) divergence to it. We start by describing phase (2) parameter optimization/clustering, given an initial estimate of θ and {µj} k j=1.
* We initialize DEC with a stacked autoencoder (SAE) because recent research has shown that they consistently produce semantically meaningful and well-separated representations on real-world datasets
* We use the standard unsupervised evaluation metric and protocols for evaluations and comparisons to other algorithms. For all algorithms we set the number of clusters to the number of ground-truth categories and evaluate performance with unsupervised clustering accuracy (ACC ): ACC = max  m  Pn  i=1 1{li = m(ci)}  n  , (10)  where li  is the ground-truth label, ci is the cluster assignment produced by the algorithm, and m ranges over all possible one-to-one mappings between clusters and labels.